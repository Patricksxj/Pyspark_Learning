{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T02:50:35.532579Z",
     "start_time": "2020-09-22T02:50:30.241786Z"
    }
   },
   "source": [
    "#### 针对直播中提到对于模型融合stacking和blending方法的使用\n",
    "直播链接：[Datawhale 零基础入门金融风控 建模调参&模型融合](https://tianchi.aliyun.com/course/live?liveId=41206)\n",
    "> 优点：可以直接使用xgb、lgb原生库进行建模，效率远高于sklearn接口下的建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用heamy模块进行模型在线融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:04:54.003468Z",
     "start_time": "2020-09-24T08:04:53.252815Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:04:54.672431Z",
     "start_time": "2020-09-24T08:04:54.005429Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "sns 相关设置\n",
    "@return:\n",
    "\"\"\"\n",
    "# 声明使用 Seaborn 样式\n",
    "sns.set()\n",
    "# 有五种seaborn的绘图风格，它们分别是：darkgrid, whitegrid, dark, white, ticks。默认的主题是darkgrid。\n",
    "sns.set_style(\"whitegrid\")\n",
    "# 有四个预置的环境，按大小从小到大排列分别为：paper, notebook, talk, poster。其中，notebook是默认的。\n",
    "sns.set_context('talk')\n",
    "# 中文字体设置-黑体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 解决保存图像是负号'-'显示为方块的问题\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# 解决Seaborn中文显示问题并调整字体大小\n",
    "sns.set(font='SimHei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预操作 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:04:54.686316Z",
     "start_time": "2020-09-24T08:04:54.674371Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum()  / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum()  / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:04:59.461692Z",
     "start_time": "2020-09-24T08:04:54.687337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.87 MB\n",
      "Memory usage after optimization is: 69.46 MB\n",
      "Decreased by 75.8%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"读取数据\"\"\"\n",
    "df_data = pd.read_csv('train.csv')\n",
    "df_data = reduce_mem_usage(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:05:00.109137Z",
     "start_time": "2020-09-24T08:04:59.463655Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"建立模型：【模型参数：xgb-->鱼佬baseline，lgb --> 贝叶斯调参】\"\"\"\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def xgb_model(X_train, y_train, X_test, y_test=None):\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    train_matrix = xgb.DMatrix(X_train_split , label=y_train_split)\n",
    "    valid_matrix = xgb.DMatrix(X_val , label=y_val)\n",
    "    test_matrix = xgb.DMatrix(X_test)\n",
    "\n",
    "    params = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'gamma': 1,\n",
    "        'min_child_weight': 1.5,\n",
    "        'max_depth': 5,\n",
    "        'lambda': 10,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'colsample_bylevel': 0.7,\n",
    "        'eta': 0.04,\n",
    "        'tree_method': 'exact',\n",
    "        'seed': 2020,\n",
    "        'n_jobs': -1,\n",
    "        \"silent\": True,\n",
    "    }\n",
    "    watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "    \n",
    "    model = xgb.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "    \"\"\"计算在验证集上的得分\"\"\"\n",
    "    val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_val, val_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print('调参后xgboost单模型在验证集上的AUC：{}'.format(roc_auc))\n",
    "    \"\"\"对测试集进行预测\"\"\"\n",
    "    test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "    \n",
    "    return test_pred\n",
    "    \n",
    "\n",
    "def lgb_model(X_train, y_train, X_test, y_test=None):\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # 调参后的最优参数\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.01,\n",
    "        'min_child_weight': 0.32,\n",
    "        'num_leaves': 14,\n",
    "        'max_depth': 4,\n",
    "        'feature_fraction': 0.81,\n",
    "        'bagging_fraction': 0.61,\n",
    "        'bagging_freq': 9,\n",
    "        'min_data_in_leaf': 13,\n",
    "        'min_split_gain': 0.27,\n",
    "        'reg_alpha': 9.58,\n",
    "        'reg_lambda': 4.62,\n",
    "        'seed': 2020,\n",
    "        'n_jobs':-1,\n",
    "        'silent': True,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], verbose_eval=500, early_stopping_rounds=500)\n",
    "    \"\"\"计算在验证集上的得分\"\"\"\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_val, val_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print('调参后lightgbm单模型在验证集上的AUC：{}'.format(roc_auc))\n",
    "    \"\"\"对测试集进行预测\"\"\"\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于模型层面的融合 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:05:00.405288Z",
     "start_time": "2020-09-24T08:05:00.111048Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sample'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c9ba43062548>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;34m\"\"\"数据集设置\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'issueDate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'isDefault'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'issueDate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'isDefault'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'isDefault'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sample'"
     ]
    }
   ],
   "source": [
    "\"\"\"对训练集数据进行划分，分成训练集和验证集，并进行相应的操作\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"数据集设置\"\"\"\n",
    "X_train = df_data.loc[df_data['sample']=='train', :].drop(['id','issueDate','isDefault'], axis=1)\n",
    "X_test = df_data.loc[df_data['sample']=='test', :].drop(['id','issueDate','isDefault'], axis=1)\n",
    "y_train = df_data.loc[df_data['sample']=='train', 'isDefault']\n",
    "# 数据集划分\n",
    "# X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:05:00.412273Z",
     "start_time": "2020-09-24T08:05:00.406255Z"
    }
   },
   "outputs": [],
   "source": [
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Classifier\n",
    "\n",
    "model_dataset = Dataset(X_train=X_train, y_train=y_train, X_test=X_test)\n",
    "model_xgb = Classifier(dataset=model_dataset, estimator=xgb_model, name='xgb', use_cache=False)\n",
    "model_lgb = Classifier(dataset=model_dataset, estimator=lgb_model, name='lgb', use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用stacking 方法进行模型融合 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T08:05:00.424207Z",
     "start_time": "2020-09-24T08:05:00.414236Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from heamy.pipeline import ModelsPipeline\n",
    "\n",
    "pipeline = ModelsPipeline(model_xgb, model_lgb)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T09:50:21.236925Z",
     "start_time": "2020-09-24T08:05:01.288941Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建第一层新特征，其中k默认是5，表示5折交叉验证，full_test=True，对全部训练集进行训练得到基学习器，然后用基学习器对测试集预测得到新特征\n",
    "stack_ds = pipeline.stack(k=5, seed=111, full_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T12:51:18.260421Z",
     "start_time": "2020-09-24T12:51:16.800773Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# 第二层使用逻辑回归进行stack\n",
    "LogisticRegression(solver='lbfgs')\n",
    "stacker = Classifier(dataset=stack_ds, estimator=LogisticRegression, parameters={'solver': 'lbfgs'})\n",
    "# 测试集的预测结果\n",
    "test_pred = stacker.predict()\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T12:51:39.844600Z",
     "start_time": "2020-09-24T12:51:39.811689Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"生成提交格式的DataFrame\"\"\"\n",
    "df_result = pd.DataFrame({'id': df_data.loc[df_data['sample']=='test', 'id'].values, 'isDefault': test_pred})\n",
    "df_result.sort_values(by='id').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T12:51:55.134404Z",
     "start_time": "2020-09-24T12:51:54.372872Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"保存数据用于预测建模\"\"\"\n",
    "df_result.to_csv('dataset/submission_data_stacking_model_20200924_V1_5folds.csv', encoding='gbk', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用blending方法进行模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:09:54.497180Z",
     "start_time": "2020-09-24T12:52:03.331634Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建第一层新特征，将训练集切分成8:2，其中80%用于训练基学习器，20%用于构建新特征\n",
    "blend_ds = pipeline.blend(proportion=0.2,seed=111)\n",
    "# 第二层使用逻辑回归进行blend\n",
    "blender = Classifier(dataset=blend_ds, estimator=LogisticRegression, parameters={'solver': 'lbfgs'})\n",
    "# 测试集的预测结果\n",
    "test_pred = blender.predict()\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:10:00.382208Z",
     "start_time": "2020-09-24T13:10:00.353552Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"生成提交格式的DataFrame\"\"\"\n",
    "df_result = pd.DataFrame({'id': df_data.loc[df_data['sample']=='test', 'id'].values, 'isDefault': test_pred})\n",
    "df_result.sort_values(by='id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:10:04.588850Z",
     "start_time": "2020-09-24T13:10:03.804489Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"保存数据用于预测建模\"\"\"\n",
    "df_result.to_csv('dataset/submission_data_blending_model_20200924_V1.csv', encoding='gbk', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "199857",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
